{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "aca5mm2gpbaks3mpxgwo",
   "authorId": "322325853055",
   "authorName": "CROMANO",
   "authorEmail": "chase.romano@snowflake.com",
   "sessionId": "46b84767-61a4-4daf-b99d-f98debfba163",
   "lastEditTime": 1754400606413
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "d5016801-b6e8-4679-b403-c3c401f3ea62",
   "metadata": {
    "language": "python",
    "name": "Imports",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# Import python packages\nimport json\nimport pandas as pd\nfrom snowflake.snowpark import functions as F\nfrom datetime import date, timedelta\nfrom snowflake.snowpark import types as T\nfrom snowflake.ml.model import custom_model\nfrom snowflake.ml.model import model_signature\nfrom snowflake.ml.registry import registry\nimport numpy as np\nimport ast\nfrom snowflake.ml.modeling.metrics import mean_absolute_percentage_error\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndef fix_values(column):\n    return F.upper(F.regexp_replace(F.col(column), \"[^a-zA-Z0-9]+\", \"_\"))\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7bef8463-84e6-4549-bc86-48b33e886a08",
   "metadata": {
    "language": "python",
    "name": "_load_csv",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Shows how to create a pandas dataframe\n# Writes a pandas dataframe to Snowflake as a table\ntspd = pd.read_csv('data/ts_data.csv')\n\nts = session.create_dataframe(tspd)\nts.write.mode(\"overwrite\").save_as_table(\"traffic_data\")\nts.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "08966818-802b-452b-8036-5832ee6d7c24",
   "metadata": {
    "language": "python",
    "name": "dynamic_dates",
    "collapsed": false,
    "resultHeight": 127,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "df = session.table(\"traffic_data\")\nfor col in [\"HOLIDAY_NAME\"]:\n    df = df.with_column(col, fix_values(col))\n\ntrain_start_date = '2021-01-01'\ntrain_end_date = '2024-10-08'\ntest_start_date = '2024-10-22'\ntest_end_date = '2024-12-05'\n\nmax_forecast_str = '2024-12-19'\n\nprint(\"Train Start Date: 2021-01-01\")\nprint(f\"Train End Date: {train_end_date}\")\nprint(f\"Test Start Date: {test_start_date}\")\nprint(f\"Test End Date: {test_end_date}\")\nprint(f\"Forecast End Date: {max_forecast_str}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "97557121-2b90-492a-9a5b-81cbc755f681",
   "metadata": {
    "language": "python",
    "name": "Stores_DATA",
    "collapsed": false,
    "resultHeight": 345,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# We will be one-hot encoding Holidays\n# For Case/space sensitivity we will fix all holidays\n\nfor col in [\"HOLIDAY_NAME\"]:\n    df = df.with_column(col, fix_values(col))\n\ndf.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1f9b03fa-37dc-49d3-81a9-aceba8da069f",
   "metadata": {
    "name": "run_one_model",
    "collapsed": false
   },
   "source": "Run One Model"
  },
  {
   "cell_type": "code",
   "id": "efe2a307-c858-4f58-9a7b-aaabac38840d",
   "metadata": {
    "language": "python",
    "name": "one_store",
    "collapsed": false
   },
   "outputs": [],
   "source": "df_features = session.table('traffic_data').filter(F.col('STORE_ID') == 1).to_pandas()\ndf_features['DATE'] = pd.to_datetime(df_features['DATE'])\ndf_features = df_features.sort_values(by='DATE', ascending=False)\ndf_features",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "09d290af-3ec9-4f6b-bb69-cbc28c774715",
   "metadata": {
    "name": "Single_Model_Run",
    "collapsed": false
   },
   "source": "For partitioned modeling I recommend getting one model to run correctly before training every partition that way you can take the code that works for one model and insert it into the partitioned modeling API"
  },
  {
   "cell_type": "code",
   "id": "c5eafcba-b1e2-4bf5-aa57-9ba457fd6406",
   "metadata": {
    "language": "python",
    "name": "one_store_model",
    "collapsed": false
   },
   "outputs": [],
   "source": "import xgboost\n\ndf = df_features\n# Set the date column as our index.\ndf['DATETIME'] = pd.to_datetime(df['DATE'])\n\n# Set the index to the datetime column\ndf.set_index('DATETIME', inplace=True)\ndf = df.drop(columns = ['DATE'])\n\nfor column in df.columns:\n    if column not in ['TRAFFIC','HOLIDAY_NAME']:\n        df[column] = df[column].astype('int')\n        \n# Use get_dummies (one-hot encoding) for categorical features.\ndf = pd.get_dummies(data=df, columns=['HOLIDAY_NAME'])\n\n\ntrain = df[(df.index >= pd.to_datetime(train_start_date)) & (df.index <= pd.to_datetime(train_end_date))]\n\nforecast = df[(df.index >= pd.to_datetime(test_start_date)) & (df.index <= pd.to_datetime(max_forecast_str))]\n\n# Remove the target from the input dataset, and construct target dataset.\nX_train = train.drop('TRAFFIC', axis=1)\ny_train = train['TRAFFIC']\n\nX_forecast = forecast.drop('TRAFFIC', axis=1)\n\n# Train an XGBoost regression model.\nmodel = xgboost.XGBRegressor(n_estimators=200, n_jobs=1)\nmodel.fit(X_train, y_train, verbose=False)\n\n# Predict the hourly forecast for the future dates and make sure no predictions are less than zero.\nforecast.loc[:, 'PREDICTION'] = model.predict(X_forecast)\nforecast['DATETIME'] = forecast.index\nforecast = forecast[['DATETIME', 'PREDICTION']]\nforecast = forecast.sort_index()\nforecast.loc[forecast['PREDICTION'] < 0, 'PREDICTION'] = 0\nforecast",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ac8b74ad-ed7f-4a3d-a248-10e9ed0920fd",
   "metadata": {
    "language": "python",
    "name": "one_store_forecast",
    "collapsed": false
   },
   "outputs": [],
   "source": "forecast.rename(columns={'DATETIME': 'DATE'}, inplace=True)\npred_vs_actual = pd.merge(df_features[['DATE', 'TRAFFIC']], forecast[['DATE', 'PREDICTION']], on='DATE', how='inner')\npred_vs_actual",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "52c7fbb6-aadf-4872-bfa3-ea1ef1f0711f",
   "metadata": {
    "language": "python",
    "name": "visualize",
    "collapsed": false
   },
   "outputs": [],
   "source": "import altair as alt\nimport streamlit as st\ndf = pred_vs_actual\n# Replace 0's in 'TRAFFIC' after '2024-11-02' with NaN\ndf.loc[(df['DATE'] > '2024-11-02') & (df['TRAFFIC'] == 0), 'TRAFFIC'] = np.nan\n\n# Set DATE as the index for better plotting\ndf.set_index('DATE', inplace=True)\n\n# Create the line chart with altair\nchart = alt.Chart(df.reset_index()).transform_fold(\n    ['TRAFFIC', 'PREDICTION'],\n    as_=['variable', 'value']\n).mark_line().encode(\n    x='DATE:T',\n    y=alt.Y('value:Q').scale(zero=False),  # Set the y-axis starting from 500\n    color='variable:N'\n).properties(\n    title='TRAFFIC and PREDICTION Line Chart'\n)\n\n# Display the chart in Streamlit\nst.altair_chart(chart, use_container_width=True)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e8b3d6c4-3b54-4097-b1cb-ed12cdf293eb",
   "metadata": {
    "name": "run_500_models",
    "collapsed": false
   },
   "source": "Run 500 models with the partitioned modeling API"
  },
  {
   "cell_type": "code",
   "id": "237abbaf-401d-478f-b43c-fa90736592d2",
   "metadata": {
    "language": "python",
    "name": "create_model",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "from snowflake.ml.model import custom_model\nclass ForecastingModel(custom_model.CustomModel):\n\n    # Use the same decorator as for methods with FUNCTION inference.\n    @custom_model.partitioned_inference_api\n    def predict(self, df: pd.DataFrame) -> pd.DataFrame:        \n        import xgboost\n        \n        # Set the date column as our index.\n        df['DATETIME'] = pd.to_datetime(df['DATE'])\n        \n        # Set the index to the datetime column\n        df.set_index('DATETIME', inplace=True)\n        df = df.drop(columns = ['DATE'])\n        \n        for column in df.columns:\n            if column not in ['TRAFFIC','HOLIDAY_NAME']:\n                df[column] = df[column].astype('int')\n                \n        # Use get_dummies (one-hot encoding) for categorical features.\n        df = pd.get_dummies(data=df, columns=['HOLIDAY_NAME'])\n        \n        \n        train = df[(df.index >= pd.to_datetime(train_start_date)) & (df.index <= pd.to_datetime(train_end_date))]\n        \n        forecast = df[(df.index >= pd.to_datetime(test_start_date)) & (df.index <= pd.to_datetime(max_forecast_str))]\n        \n        # Remove the target from the input dataset, and construct target dataset.\n        X_train = train.drop('TRAFFIC', axis=1)\n        y_train = train['TRAFFIC']\n        \n        X_forecast = forecast.drop('TRAFFIC', axis=1)\n        \n        # Train an XGBoost regression model.\n        model = xgboost.XGBRegressor(n_estimators=200, n_jobs=1)\n        model.fit(X_train, y_train, verbose=False)\n        \n        # Predict the hourly forecast for the future dates and make sure no predictions are less than zero.\n        forecast.loc[:, 'PREDICTION'] = model.predict(X_forecast)\n        forecast['DATETIME'] = forecast.index\n        forecast = forecast[['DATETIME', 'PREDICTION']]\n        forecast = forecast.sort_index()\n        forecast.loc[forecast['PREDICTION'] < 0, 'PREDICTION'] = 0\n\n        return forecast\n\nmy_forecasting_model = ForecastingModel()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "14edd498-7ef4-4d70-a994-64ec8734f63c",
   "metadata": {
    "language": "python",
    "name": "get_model_version",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# This is just a nice little fucntion to automatically\n# Make versions iterate V_1, V_2\n\ndef get_next_version(reg, model_name) -> str:\n    \"\"\"\n    Returns the next version of a model based on the existing versions in the registry.\n\n    Args:\n        reg: The registry object that provides access to the models.\n        model_name: The name of the model.\n\n    Returns:\n        str: The next version of the model in the format \"V_<version_number>\".\n\n    Raises:\n        ValueError: If the version list for the model is empty or if the version format is invalid.\n    \"\"\"\n    models = reg.show_models()\n    if models.empty:\n        return \"V_1\"\n    elif model_name not in models[\"name\"].to_list():\n        return \"V_1\"\n    max_version_number = max(\n        [\n            int(version.split(\"_\")[-1])\n            for version in ast.literal_eval(\n                models.loc[models[\"name\"] == model_name, \"versions\"].values[0]\n            )\n        ]\n    )\n    return f\"V_{max_version_number + 1}\"",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "881ccddf-574b-4541-827b-66f8839d3016",
   "metadata": {
    "language": "python",
    "name": "register_model",
    "collapsed": false,
    "resultHeight": 169
   },
   "outputs": [],
   "source": "reg = registry.Registry(session=session)\nmodel_name = \"XGB_TRAFFIC\"\noptions = {\n    \"function_type\": \"TABLE_FUNCTION\",\n}\n\nmv = reg.log_model(\n    my_forecasting_model,\n    model_name=model_name,\n    version_name=get_next_version(reg,model_name),\n    conda_dependencies=[\"pandas\", \"scikit-learn\", \"xgboost\"],\n    options=options,\n    signatures={\n        \"predict\": model_signature.ModelSignature(\n            inputs=[\n                model_signature.FeatureSpec(name=\"DATE\", dtype=model_signature.DataType.TIMESTAMP_NTZ),\n                model_signature.FeatureSpec(name=\"STORE_ID\", dtype=model_signature.DataType.INT64),\n                model_signature.FeatureSpec(name=\"WEEK_DAY_NBR\", dtype=model_signature.DataType.INT64),\n                model_signature.FeatureSpec(name=\"MTH_DAY_NBR\", dtype=model_signature.DataType.INT64),\n                model_signature.FeatureSpec(name=\"CALENDAR_MTH\", dtype=model_signature.DataType.INT64),\n                model_signature.FeatureSpec(name=\"CALENDAR_YEAR\", dtype=model_signature.DataType.INT64),\n                model_signature.FeatureSpec(name=\"HOLIDAY_NAME\", dtype=model_signature.DataType.STRING),\n                model_signature.FeatureSpec(name=\"TRAFFIC\", dtype=model_signature.DataType.DOUBLE),\n            ],\n            outputs=[\n                model_signature.FeatureSpec(name=\"DATETIME\", dtype=model_signature.DataType.TIMESTAMP_NTZ),\n                model_signature.FeatureSpec(name=\"PREDICTION\", dtype=model_signature.DataType.DOUBLE),\n         ],\n        )\n    },\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9eecda72-6dcc-4f5b-8157-3ee9a83f62fb",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": "df = session.table(\"traffic_data\")\nfor col in [\"HOLIDAY_NAME\"]:\n    df = df.with_column(col, fix_values(col))\n\ndf.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "76bc1250-903d-45e2-9936-0ac75022aafd",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "df = session.table(\"traffic_data\").with_column(\"DATE\", F.to_timestamp(F.col(\"DATE\"))).cache_result()\nfor col in [\"HOLIDAY_NAME\"]:\n    df = df.with_column(col, fix_values(col))\n\nresults = mv.run(df, partition_column=\"STORE_ID\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "415fdc80-6cbd-4ee6-a3e0-878da2ac4b86",
   "metadata": {
    "language": "python",
    "name": "run_model",
    "collapsed": false,
    "resultHeight": 345
   },
   "outputs": [],
   "source": "df = session.table(\"traffic_data\").with_column(\"DATE\", F.to_timestamp(F.col(\"DATE\")))\nfor col in [\"HOLIDAY_NAME\"]:\n    df = df.with_column(col, fix_values(col))\n\nresults = mv.run(df, partition_column=\"STORE_ID\").select(\n    F.to_date(\"DATETIME\").alias(\"DATE\"),\"STORE_ID\",\"PREDICTION\"\n)\n\n\nresults.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2c4b1528-a34a-4459-b68a-42db9beb82a0",
   "metadata": {
    "language": "python",
    "name": "combine_pred_act",
    "collapsed": false,
    "resultHeight": 329
   },
   "outputs": [],
   "source": "final_XGB = results.join(\n    df, ['DATE','STORE_ID']\n).select(\n    F.to_date(df.DATE).alias(\"DATE\"),\n    df.STORE_ID.alias(\"STORE_ID\"),\n    df.TRAFFIC,\n    results.PREDICTION,\n).with_column('MODEL',F.lit('XGBOOST'))\nfinal_XGB.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1e93220f-ef90-4e40-a05e-52e2028fbf7b",
   "metadata": {
    "language": "python",
    "name": "MAPE_Formula",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "import json\nfrom snowflake.snowpark import functions as F\n\ndef calculate_mape(df):\n    \"\"\"\n    Calculate the Mean Absolute Percentage Error (MAPE) between actual and predicted values for each country and channel using Snowpark.\n\n    Parameters:\n    df (DataFrame): Snowpark DataFrame containing actual and predicted values.\n\n    Returns:\n    DataFrame: DataFrame containing the MAPE value as a percentage for each country and channel.\n    \"\"\"\n    # Create a temporary column for the absolute percentage error\n    mape_df = df.with_column(\n        \"APE\",\n        F.div0(F.abs(F.col('TRAFFIC') - F.col('PREDICTION')), F.col('TRAFFIC'))\n    )\n\n    # Group by COUNTRY and CHANNEL and calculate the mean of the absolute percentage error\n    mape_per_store_id = mape_df.group_by([\"STORE_ID\"]).agg(\n        (F.mean(\"APE\") * 100).alias(\"MAPE\")\n    )\n\n    return mape_per_store_id",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d9999c14-e2de-4b05-aa65-957fb8f2d44b",
   "metadata": {
    "language": "python",
    "name": "register_mape",
    "collapsed": false,
    "resultHeight": 284
   },
   "outputs": [],
   "source": "mape_values = calculate_mape(\n    final_XGB.filter(\n        (F.col(\"DATE\") >= test_start_date) & (F.col(\"DATE\") <= test_end_date) & (F.col(\"STORE_ID\") <= 10)\n    )\n)\nmape_results = mape_values.collect()\n\n# Create a nested dictionary with COUNTRY as the outer key and CHANNEL as the inner key\nmape_dict = {}\nfor row in mape_results:\n    store_id = row[0]\n    mape_value = round(row[1], 1)\n    if store_id not in mape_dict:\n        mape_dict[store_id] = {}\n    mape_dict[store_id] = f\"{mape_value}%\"\n\nmape_json = json.dumps(mape_dict, indent=4)\nprint(mape_json)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a1d5e9a5-5759-4ab9-ab1a-2838053b1885",
   "metadata": {
    "language": "python",
    "name": "add_mape_to_registry",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "mv.set_metric(metric_name=\"MAPES\", value=mape_json)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fadc3643-ceda-4660-9ef6-03433329625c",
   "metadata": {
    "language": "python",
    "name": "show_models",
    "collapsed": false,
    "resultHeight": 147
   },
   "outputs": [],
   "source": "reg.show_models()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7ef3c7e6-e4c0-467d-b987-16e69071fdc2",
   "metadata": {
    "language": "python",
    "name": "prophet_class",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "from snowflake.ml.model import custom_model\nclass ForecastingModel_prophet(custom_model.CustomModel):\n\n    # Use the same decorator as for methods with FUNCTION inference.\n    @custom_model.partitioned_inference_api\n    def predict(self, df: pd.DataFrame) -> pd.DataFrame:        \n        from prophet import Prophet\n        \n        \n        for column in df.columns:\n            if column not in ['TRAFFIC','HOLIDAY_NAME','DATE']:\n                df[column] = df[column].astype('int')\n        \n        holidays_df = df[['DATE', 'HOLIDAY_NAME']].rename(columns={'DATE': 'ds', 'HOLIDAY_NAME': 'holiday'})\n\n        df = df.drop(columns = ['STORE_ID','HOLIDAY_NAME'])\n        \n        # # Filter the DataFrame\n        train = df[(df['DATE'] >= train_start_date) & (df['DATE'] <= train_end_date)]\n        test = df[(df['DATE'] >= test_start_date) & (df['DATE'] <= max_forecast_str)]\n        \n        # # Prepare data for Prophet\n        prophet_df = train[['DATE', 'TRAFFIC']].rename(columns={'DATE': 'ds', 'TRAFFIC': 'y'})\n        \n        # List of extra features to add\n        extra_features = [col for col in train.columns if col not in ['TRAFFIC']]\n        extra_feature_df = train[extra_features].reset_index(drop=True)\n        \n        extra_feature_df.head()\n        #Concatenate the extra features with the prophet_df\n        prophet_df = pd.merge(prophet_df, extra_feature_df, left_on='ds', right_on='DATE', how='inner')\n        \n        # Initialize the Prophet model\n        prophet_model = Prophet(holidays = holidays_df)\n        \n        # Add each extra feature as a regressor\n        for feature in extra_features:\n            prophet_model.add_regressor(feature)\n        \n        # Fit the model\n        prophet_model.fit(prophet_df)\n        \n        # Create future DataFrame for Prophet\n        future = prophet_model.make_future_dataframe(periods=len(test))\n        \n        # Add the extra features to the future DataFrame\n        for feature in extra_features:\n            future[feature] = pd.concat([train[feature], test[feature]]).reset_index(drop=True)\n        \n        # Get the Prophet predictions\n        prophet_forecast = prophet_model.predict(future)\n        \n        # Extract Prophet's predictions for the test period\n        prophet_test_forecast = prophet_forecast['yhat'].iloc[-len(test):].reset_index(drop=True)\n        \n        # Create DataFrame for Prophet results\n        prophet_results = pd.DataFrame({\n            'DATE': future['ds'].iloc[-len(test):].reset_index(drop=True),  # Use 'ds' for the future dates\n            'Prophet_Prediction': prophet_test_forecast\n        })\n        \n        # Ensure DATE is in datetime format\n        prophet_results['DATE'] = pd.to_datetime(prophet_results['DATE'])\n\n        return prophet_results\n\nmy_forecasting_model_prophet = ForecastingModel_prophet()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9e6980dd-5fd1-4311-bcac-c46df89a8d59",
   "metadata": {
    "language": "python",
    "name": "Register_Prophet",
    "collapsed": false,
    "resultHeight": 169
   },
   "outputs": [],
   "source": "model_name=\"PROPHET\"\noptions = {\n    \"function_type\": \"TABLE_FUNCTION\",\n}\n\nmv = reg.log_model(\n    my_forecasting_model_prophet,\n    model_name=model_name,\n    version_name=get_next_version(reg,model_name),\n    conda_dependencies=[\"pandas\", \"scikit-learn\", \"xgboost\",\"prophet\"],\n    options=options,\n    signatures={\n        \"predict\": model_signature.ModelSignature(\n            inputs=[\n                model_signature.FeatureSpec(name=\"DATE\", dtype=model_signature.DataType.TIMESTAMP_NTZ),\n                model_signature.FeatureSpec(name=\"STORE_ID\", dtype=model_signature.DataType.INT64),\n                model_signature.FeatureSpec(name=\"WEEK_DAY_NBR\", dtype=model_signature.DataType.INT64),\n                model_signature.FeatureSpec(name=\"MTH_DAY_NBR\", dtype=model_signature.DataType.INT64),\n                model_signature.FeatureSpec(name=\"CALENDAR_MTH\", dtype=model_signature.DataType.INT64),\n                model_signature.FeatureSpec(name=\"CALENDAR_YEAR\", dtype=model_signature.DataType.INT64),\n                model_signature.FeatureSpec(name=\"HOLIDAY_NAME\", dtype=model_signature.DataType.STRING),\n                model_signature.FeatureSpec(name=\"TRAFFIC\", dtype=model_signature.DataType.DOUBLE),\n            ],\n            outputs=[\n                model_signature.FeatureSpec(name=\"DS\", dtype=model_signature.DataType.TIMESTAMP_NTZ),\n                model_signature.FeatureSpec(name=\"PREDICTION\", dtype=model_signature.DataType.DOUBLE),\n         ],\n        )\n    },\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aeab5dd6-0e3f-45bd-8fc5-67598cbed655",
   "metadata": {
    "language": "python",
    "name": "PROPHET_RESULTS",
    "collapsed": false,
    "resultHeight": 1237
   },
   "outputs": [],
   "source": "results = mv.run(df, partition_column=\"STORE_ID\").select(\n    F.to_date(\"DS\").as_('DATE'), \"PREDICTION\", \"STORE_ID\"\n).cache_result()\n\nfinal_PROPHET = results.join(\n    df, ['Date','Store_ID']\n).select(\n    F.to_date(df.DATE).alias(\"DATE\"),\n    df.STORE_ID.alias(\"STORE_ID\"),\n    df.TRAFFIC,\n    results.PREDICTION,\n).with_column('MODEL',F.lit('PROPHET'))\nfinal_PROPHET.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c12ad030-1b2f-4191-b44a-f14514812401",
   "metadata": {
    "language": "python",
    "name": "PROPHET_METRICS",
    "collapsed": false,
    "resultHeight": 284
   },
   "outputs": [],
   "source": "mape_values = calculate_mape(\n    final_PROPHET.filter(\n        (F.col(\"DATE\") >= test_start_date) & (F.col(\"DATE\") <= test_end_date) & (F.col(\"STORE_ID\") <= 10)\n    )\n)\nmape_results = mape_values.collect()\n\n# Create a nested dictionary with COUNTRY as the outer key and CHANNEL as the inner key\nmape_dict = {}\nfor row in mape_results:\n    store_id = row[0]\n    mape_value = round(row[1], 1)\n    if store_id not in mape_dict:\n        mape_dict[store_id] = {}\n    mape_dict[store_id] = f\"{mape_value}%\"\n\nmape_json = json.dumps(mape_dict, indent=4)\nprint(mape_json)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "779e9414-2ead-44c1-a4b5-ae670eeca59a",
   "metadata": {
    "language": "python",
    "name": "Save_Metrics",
    "collapsed": false,
    "resultHeight": 239
   },
   "outputs": [],
   "source": "mv.set_metric(metric_name=\"MAPES\", value=mape_json)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8841d012-3e9a-40a7-b291-1780ecf6f50e",
   "metadata": {
    "language": "python",
    "name": "write_to_snowflake",
    "collapsed": false,
    "resultHeight": 239
   },
   "outputs": [],
   "source": "final = final_PROPHET.union(final_XGB)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2dc55f7a-1fa8-4a48-9794-158a1056d16c",
   "metadata": {
    "language": "python",
    "name": "create_xgb_prophet_cols",
    "collapsed": false,
    "resultHeight": 351
   },
   "outputs": [],
   "source": "from snowflake.snowpark.functions import col\n# Create DataFrames for XGBOOST and PROPHET models\ndf_xgb = (\n    final\n    .filter(col(\"MODEL\") == \"XGBOOST\")\n    .select(\n        \"*\", \n        (col(\"PREDICTION\").alias(\"PRED_XGB\"))\n    )\n).cache_result()\n\ndf_prophet = (\n    final\n    .filter(col(\"MODEL\") == \"PROPHET\")\n    .select(\n        \"*\", \n        (col(\"PREDICTION\").alias(\"PRED_PROPHET\"))\n    )\n).cache_result()\n\n# Join the two DataFrames\nresult_df = (\n    df_xgb.join(\n        df_prophet,\n        (df_xgb[\"DATE\"] == df_prophet[\"DATE\"]) &\n        (df_xgb[\"STORE_ID\"] == df_prophet[\"STORE_ID\"]),\n        \"inner\"\n    )\n    .select(\n        df_xgb[\"DATE\"].alias('DATE'),\n        df_xgb[\"STORE_ID\"].alias('STORE_ID'),\n        df_xgb[\"TRAFFIC\"].alias('TRAFFIC'),\n        df_xgb[\"PRED_XGB\"],\n        df_prophet[\"PRED_PROPHET\"]\n    )\n)\nresult_df.write.mode(\"overwrite\").save_as_table('final_store_predictions')\nresult_df.show()",
   "execution_count": null
  }
 ]
}